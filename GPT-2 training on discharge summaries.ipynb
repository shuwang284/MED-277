{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4dae1420",
      "metadata": {
        "id": "4dae1420"
      },
      "source": [
        "# Training GPT from Scratch on Discharge Summaries\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this lab, we will walk through a slightly modified version of this huggingface tutorial on training GPT from scratch: https://huggingface.co/learn/llm-course/en/chapter7/6\n",
        "\n",
        "We'll explore how training on a small corpus of discharge summaries affects the embeddings and generations of our model.\n",
        "\n",
        "We'll primarily use the higher level \"transformers\" API for this exercise. \n",
        "\n",
        "This notebook runs in colab, connect to a T4 runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fUdJrc-Up5kH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fUdJrc-Up5kH",
        "outputId": "0860ae27-b991-4cd4-a8e3-524a4e57ac14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9c8e2ce0",
      "metadata": {
        "id": "9c8e2ce0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import math\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbcbb7b",
      "metadata": {
        "id": "abbcbb7b"
      },
      "source": [
        "First, we'll load the data that we used in Lab 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NK9G-FnTq0vF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "NK9G-FnTq0vF",
        "outputId": "33b45c82-c659-4593-92d6-0e50a5f54404"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b237762-75b4-42e5-ba47-ccab8439ccac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b237762-75b4-42e5-ba47-ccab8439ccac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving lab2-data.csv to lab2-data (1).csv\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ffa99da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3ffa99da",
        "outputId": "34b9b8ce-6b2d-4de2-d0ad-f96396f741de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['HADM_ID', 'TEXT'],\n",
            "    num_rows: 3668\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "discharge_summaries = pd.read_csv('lab2-data.csv')\n",
        "dataset = Dataset.from_pandas(discharge_summaries)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090f2279",
      "metadata": {
        "id": "090f2279"
      },
      "source": [
        "Here, we'll use a pre-trained tokenizer. We'll also restrict our context length to 512. The tokenizer class allows us to break our large discharge summaries into smaller chunks that fit into our context limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "49a179a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49a179a6",
        "outputId": "3a6df285-0859-4359-f0f0-929fdd8f96e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs length: 73\n",
            "Input chunk lengths: [512, 512, 512, 512, 512, 512, 512, 512, 278, 512, 512, 512, 512, 512, 512, 512, 512, 512, 207, 512, 512, 512, 512, 512, 512, 512, 512, 315, 512, 110, 512, 512, 512, 512, 295, 512, 512, 512, 512, 512, 512, 512, 512, 475, 512, 512, 512, 512, 512, 115, 512, 512, 512, 512, 512, 512, 110, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 196, 512, 512, 512, 512, 437]\n",
            "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9]\n"
          ]
        }
      ],
      "source": [
        "context_length = 512\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    dataset[:10][\"TEXT\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f8222e",
      "metadata": {
        "id": "30f8222e"
      },
      "source": [
        "Each chunk will be list of indices corresponding to tokens in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "553036eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "553036eb",
        "outputId": "2abd3e24-413b-4560-d56f-aa38dcbfe0d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2782, 3411, 7536, 25, 220, 685, 1174, 17, 11623, 12, 17, 12, 24, 1174, 60, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 3167, 10136, 7536, 25, 220, 220, 685, 1174, 17, 11623, 12, 17, 12, 1433, 1174, 60, 628, 198, 16177, 25, 26112, 2149, 8881, 198, 198, 3237, 6422, 444, 25, 198, 57, 420, 273, 1220, 406, 3798, 349, 198, 198, 8086, 1571, 33250, 1174, 37564, 4586, 6530, 1248, 3553, 1174, 60, 198, 23675, 20011, 2913, 25, 198, 45170, 2356, 198, 198, 24206, 311, 31839, 393, 10001, 17443, 34997, 25, 198, 30645, 8710, 516, 1627, 36075, 357, 3506, 5387, 45808, 934, 27208, 8, 198, 198, 18122, 286, 21662, 5821, 1108, 25, 198, 5246, 13, 685, 1174, 29870, 938, 3672, 1248, 3365, 1174, 60, 318, 281, 9508, 27406, 582, 351, 10768, 257, 419, 291, 45219, 5958, 357, 43435, 198, 49257, 9809, 287, 685, 1174, 17, 17464, 1174, 60, 351, 685, 1174, 14749, 357, 403, 8, 16003, 1174, 60, 352, 12067, 17, 11, 31312, 2579, 8085, 39, 70, 11, 10768, 198, 2781, 1373, 842, 3686, 3780, 11, 11607, 257, 419, 291, 1035, 1648, 19777, 828, 10726, 1364, 198, 1151, 41001, 827, 301, 4160, 2612, 5287, 351, 33685, 1679, 12, 1270, 7441, 37454, 11, 198, 49229, 40712, 312, 22859, 11, 12593, 33748, 17506, 11, 37292, 264, 14, 79, 327, 6242, 38, 287, 685, 1174, 1238, 2079, 1174, 60, 351, 198, 50, 43490, 12, 43, 2885, 12, 18683, 27923, 11, 45809, 12, 2662, 11, 290, 45809, 12, 20031, 5631, 12, 49, 6489, 11, 351, 257, 302, 12, 4598, 327, 6242, 38, 287, 198, 58, 1174, 24, 12, 14, 17, 17657, 1174, 60, 351, 27564, 32, 12, 43, 2885, 11, 45809, 12, 2662, 11, 45809, 12, 10989, 27923, 11, 290, 45809, 12, 7397, 32, 13, 679, 635, 198, 10134, 6049, 25514, 30675, 498, 4369, 264, 14, 79, 25514, 17286, 198, 11793, 7076, 13, 679, 5545, 284, 685, 1174, 39, 3531, 1478, 4524, 1174, 60, 9256, 13793, 428, 3329, 351, 198, 19509, 1108, 286, 8033, 290, 7721, 2356, 290, 373, 1043, 284, 307, 287, 2612, 198, 32165, 495, 13, 198, 198, 1544, 2585, 339, 373, 287, 465, 6678, 1181, 286, 1535, 1566, 838, 25, 1270, 938, 198, 10197, 278, 618, 339, 19092, 510, 4203, 4692, 26, 352, 1711, 1568, 339, 4166, 198, 47189, 284, 6049, 7786, 7721, 2356, 19772, 803, 1973, 465, 7721, 198, 32852, 351, 32122, 11, 2566, 6570, 2850, 271, 11, 290, 20268, 862, 39718, 13, 383, 2356, 373, 198, 22043, 306, 6937, 290, 750, 407, 10568, 1566, 339, 373, 1813, 264, 43, 24563, 38, 379, 198, 21, 716, 416, 41363, 13, 679, 468, 587, 2356, 1479, 1201, 13, 21662, 278, 410, 8321, 20997, 198, 14454, 14, 2791, 11, 15172, 9166, 11, 440, 17, 3332, 9193, 4, 319, 17926, 13, 327, 55, 49, 3751, 22791, 425, 2612, 198, 32165, 495, 26, 4238, 14673, 261, 259, 12, 40, 373, 33544, 15321, 379, 657, 13, 19, 11, 45233, 4317, 13, 198, 1544, 1813, 49550, 290, 277, 1434, 43616, 485, 4019, 10527, 8363, 357, 4480, 5299, 8054, 535, 2566, 942, 271, 828, 198]\n"
          ]
        }
      ],
      "source": [
        "print(outputs['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5741ec77",
      "metadata": {
        "id": "5741ec77"
      },
      "source": [
        "We can decode this back to text as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "32ff3639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "32ff3639",
        "outputId": "2bc98356-1d30-419d-c85d-897868125f22"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Admission Date:  [**2125-2-9**]              Discharge Date:   [**2125-2-16**]\\n\\n\\nService: MEDICINE\\n\\nAllergies:\\nZocor / Lescol\\n\\nAttending:[**Doctor Last Name 1857**]\\nChief Complaint:\\nChest pain\\n\\nMajor Surgical or Invasive Procedure:\\nCentral venous line insertion (right internal jugular vein)\\n\\nHistory of Present Illness:\\nMr. [**Known lastname 1858**] is an 84 yo man with moderate aortic stenosis (outside\\nhospital echo in [**2124**] with [**Location (un) 109**] 1 cm2, gradient 28 mmHg, moderate\\nmitral regurgitation, mild aortic insufficiency), chronic left\\nventricular systolic heart failure with EF 25-30%, hypertension,\\nhyperlipidemia, diabetes mellitus, CAD s/p CABG in [**2099**] with\\nSVG-LAD-Diagonal, SVG-OM, and SVG-RPDA-RPL, with a re-do CABG in\\n[**9-/2117**] with LIMA-LAD, SVG-OM, SVG-diagonal, and SVG-RCA. He also\\nhas severe peripheral arterial disease s/p peripheral bypass\\nsurgery. He presented to [**Hospital 1474**] Hospital ER this morning with\\nshortness of breath and chest pain and was found to be in heart\\nfailure.\\n\\nHe states he was in his usual state of health until 10:30 last\\nevening when he woke up feeling cold; 1 hour later he developed\\nmoderate to severe sharp chest pain radiating across his chest\\nassociated with nausea, diaphoresis, and dypsnea. The pain was\\nfairly constant and did not resolve until he was given sL NTG at\\n6 am by EMS. He has been pain free since. Presenting vitals BP\\n109/66, HR 71, O2 sat 88% on RA. CXR showed congestive heart\\nfailure; initial troponin-I was mildly elevated at 0.4, CK 70.\\nHe given aspirin and furosemide 80 mg IV (with ~600cc diuresis),\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(outputs['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f6a26d",
      "metadata": {
        "id": "e2f6a26d"
      },
      "source": [
        "Let's now apply the tokenizer across the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ac5f6f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9184b51614c24be68cff97eddd5db38f",
            "d9764e73924147a29464f09db0d168ec",
            "033d47b1f05546139546b3657093b143",
            "aace702e4a6745f88abbf3b0d5768990",
            "8291003794e34c969830ce0b5f88ec8f",
            "33ad13fd4b4a40aa9f5eac0331317633",
            "34e2c7506bf34a768b0818e883964c3d",
            "c3422602e7c847aaa1a108d5bcdcb883",
            "b0a9bca42f4547fdaeb874d0fce6fdf7",
            "3eab694a9ab14e87bd3ada7a9b5f4d23",
            "dd3c90bd54ed4a27a70596130b78721f"
          ]
        },
        "id": "ac5f6f31",
        "outputId": "960d491d-97c3-4529-966d-4db59e4e092d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9184b51614c24be68cff97eddd5db38f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"TEXT\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    return {\"input_ids\": outputs['input_ids']}\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize, batched=True, remove_columns=dataset.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab4e574",
      "metadata": {
        "id": "cab4e574"
      },
      "source": [
        "Now we'll load in a randomly initialized model with the GPT2 architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8b7916dc",
      "metadata": {
        "id": "8b7916dc"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    n_layer=6,\n",
        "    n_head=6\n",
        ")\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d79db4",
      "metadata": {
        "id": "87d79db4"
      },
      "source": [
        "Let's explore this architecture a bit further:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "520c34c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "520c34c2",
        "outputId": "9ad16bde-17a6-4ba4-fdf1-4580a9150b1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b76c7c",
      "metadata": {
        "id": "e4b76c7c"
      },
      "source": [
        "WTE (Word Token Embeddings) maps each vocabulary token to a 768-dimensional semantic vector, while WPE (Word Position Embeddings) encodes each token’s position in the sequence so the model can capture word order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b3ee68",
      "metadata": {
        "id": "25b3ee68"
      },
      "source": [
        "We can can search for similar tokens using vector distances in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fdf83526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fdf83526",
        "outputId": "c0b295fa-1abe-46d3-dc4c-04a7820de715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' snipers', 'Forge', 'angel', ' joint', ']:', ' infused', ' exc', ' Ern', ' reconcile']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.encode('hospital')\n",
        "result_vector = model.transformer.wte.weight[tokens].mean(axis=0)\n",
        "\n",
        "similarities = F.cosine_similarity(\n",
        "    result_vector,\n",
        "    model.transformer.wte.weight\n",
        ")\n",
        "top_indices = similarities.topk(10).indices\n",
        "print([tokenizer.decode(idx) for idx in top_indices if idx not in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f7e8bcf",
      "metadata": {
        "id": "0f7e8bcf"
      },
      "source": [
        "These tokens are not quite related to “hospital”.\n",
        "\n",
        "Because we are using the raw word embedding table that is not trained to make similar words close to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452bf39d",
      "metadata": {
        "id": "452bf39d"
      },
      "source": [
        "# Use this model to generate some text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6dfd6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ea6dfd6c",
        "outputId": "a39c4694-1cee-448c-8529-4f02208e8966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[50256, 22556, 22556, 22556, 22556, 49032, 49032, 49032, 41491, 20589,\n",
            "         20589, 20589, 20589, 20589, 20589, 20589, 38640, 38640, 38640, 38640,\n",
            "         27216, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480,\n",
            "         31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480,\n",
            "         31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 31480, 32582,\n",
            "         32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582,\n",
            "         32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582,\n",
            "         32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582, 32582,\n",
            "         32582, 32582, 32582, 32582, 10143, 16913, 16913, 16913, 16913, 16913,\n",
            "         16913, 16913, 16913, 16913, 16913, 16913, 16913, 16913, 16913, 16913]])\n",
            "<|endoftext|>yyyyyyyy announcer announcer announcer successors wicked wicked wicked wicked wicked wicked wicked binaries binaries binaries binaries cubic Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Eh Ehhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesionhesion fails multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer multiplayer\n"
          ]
        }
      ],
      "source": [
        "output=model.generate(max_length=100)\n",
        "print(output)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yKZT92folz85",
      "metadata": {
        "id": "yKZT92folz85"
      },
      "source": [
        "The output is repeating and meaningless."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787cf6e1",
      "metadata": {
        "id": "787cf6e1"
      },
      "source": [
        "# Forward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbec172",
      "metadata": {
        "id": "acbec172"
      },
      "source": [
        "Let's pass one of our input vectors in and explore the outputs.\n",
        "\n",
        "First, we'll pass our tokenized input and retrieve the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d9012afd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d9012afd",
        "outputId": "37782d9f-fcec-4d9d-98e7-56935f821c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0089,  0.0089,  0.0566,  ...,  0.0097,  0.0179, -0.0189],\n",
            "         [ 0.0215,  0.0145,  0.0035,  ...,  0.0365,  0.0021,  0.0020],\n",
            "         [ 0.0201,  0.0087,  0.0410,  ..., -0.0274,  0.0130,  0.0029],\n",
            "         ...,\n",
            "         [-0.0414,  0.0022,  0.0016,  ...,  0.0114, -0.0440, -0.0103],\n",
            "         [ 0.0062, -0.0015,  0.0366,  ..., -0.0113,  0.0155, -0.0396],\n",
            "         [-0.0095, -0.0323,  0.0049,  ...,  0.0196, -0.0068,  0.0189]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "tokenized_input = torch.tensor(outputs['input_ids'][0:1])\n",
        "token_embeddings = model.transformer.wte(tokenized_input)\n",
        "print(token_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8b3a09",
      "metadata": {
        "id": "1d8b3a09"
      },
      "source": [
        "# Generate the position embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "83b7b433",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "83b7b433",
        "outputId": "418c2ef4-04b4-4ed4-d5a2-cba3eb08dcc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0029, -0.0111,  0.0005,  ...,  0.0163, -0.0182,  0.0263],\n",
            "         [-0.0074, -0.0013, -0.0127,  ...,  0.0270,  0.0201,  0.0368],\n",
            "         [-0.0191, -0.0167, -0.0087,  ..., -0.0213, -0.0149,  0.0073],\n",
            "         ...,\n",
            "         [ 0.0194,  0.0234,  0.0026,  ...,  0.0251, -0.0051, -0.0079],\n",
            "         [ 0.0032,  0.0075,  0.0110,  ..., -0.0017,  0.0375, -0.0280],\n",
            "         [-0.0149, -0.0069,  0.0313,  ..., -0.0061, -0.0156, -0.0316]]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    seq_len = tokenized_input.size(1)\n",
        "    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
        "    position_embeddings = model.transformer.wpe(position_ids)\n",
        "\n",
        "print(position_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2ce13e",
      "metadata": {
        "id": "2c2ce13e"
      },
      "source": [
        "# Combine the position and token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92db8895",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "92db8895",
        "outputId": "b5f4aad5-b122-4918-e176-8fecd88f7d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0060, -0.0022,  0.0571,  ...,  0.0260, -0.0004,  0.0074],\n",
            "         [ 0.0141,  0.0132, -0.0093,  ...,  0.0636,  0.0223,  0.0388],\n",
            "         [ 0.0009, -0.0080,  0.0323,  ..., -0.0487, -0.0020,  0.0102],\n",
            "         ...,\n",
            "         [-0.0220,  0.0257,  0.0041,  ...,  0.0365, -0.0492, -0.0182],\n",
            "         [ 0.0094,  0.0060,  0.0476,  ..., -0.0130,  0.0529, -0.0676],\n",
            "         [-0.0244, -0.0391,  0.0362,  ...,  0.0135, -0.0224, -0.0127]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inputs_embeds = token_embeddings + position_embeddings\n",
        "print(inputs_embeds)\n",
        "outputs = model(inputs_embeds=inputs_embeds, output_hidden_states=True)\n",
        "hidden_states_all = outputs.hidden_states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1dzHf_JfKsGH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1dzHf_JfKsGH",
        "outputId": "880435be-924f-488b-d870-6ebe39717438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[[-0.0035, -0.0148,  0.0639,  ...,  0.0469, -0.0207,  0.0375],\n",
            "         [ 0.0074,  0.0131, -0.0244,  ...,  0.0000,  0.0471,  0.0841],\n",
            "         [-0.0202, -0.0274,  0.0263,  ..., -0.0779, -0.0188,  0.0194],\n",
            "         ...,\n",
            "         [-0.0029,  0.0546,  0.0075,  ...,  0.0685, -0.0603, -0.0291],\n",
            "         [ 0.0140,  0.0150,  0.0651,  ..., -0.0164,  0.1005, -0.1062],\n",
            "         [-0.0000, -0.0511,  0.0750,  ...,  0.0083, -0.0421, -0.0491]]],\n",
            "       grad_fn=<MulBackward0>), tensor([[[ 0.0027, -0.1910,  0.4467,  ...,  0.3890, -0.2522, -0.0575],\n",
            "         [-0.0132, -0.0493,  0.2402,  ...,  0.3599,  0.3264,  0.2404],\n",
            "         [ 0.1071, -0.1741,  0.2694,  ...,  0.1385, -0.0628, -0.1560],\n",
            "         ...,\n",
            "         [ 0.1630, -0.0845,  0.0721,  ...,  0.1247,  0.1242, -0.2141],\n",
            "         [ 0.0289, -0.0574,  0.1486,  ..., -0.0137,  0.2151, -0.1069],\n",
            "         [-0.1946, -0.2816,  0.1755,  ...,  0.1111, -0.1591, -0.0589]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.2531, -0.3715,  0.2102,  ...,  0.3261, -0.4136, -0.2686],\n",
            "         [ 0.2248, -0.2694,  0.1997,  ...,  0.3155,  0.2229,  0.0314],\n",
            "         [ 0.3621, -0.1833,  0.2503,  ...,  0.0773,  0.1443, -0.3553],\n",
            "         ...,\n",
            "         [ 0.3843,  0.0475,  0.1671,  ...,  0.1689,  0.3194, -0.2240],\n",
            "         [ 0.1918, -0.0463,  0.2881,  ..., -0.0937,  0.2232, -0.2148],\n",
            "         [-0.0422, -0.1540,  0.3612,  ..., -0.0201, -0.2007, -0.1707]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.2867, -0.4531, -0.0645,  ...,  0.4851, -0.5495, -0.2435],\n",
            "         [ 0.4606, -0.3373,  0.2293,  ...,  0.4313,  0.2229, -0.0473],\n",
            "         [ 0.4424, -0.1910,  0.0674,  ...,  0.1177,  0.0974, -0.4116],\n",
            "         ...,\n",
            "         [ 0.3738, -0.0942,  0.1191,  ...,  0.2656,  0.2497, -0.2097],\n",
            "         [ 0.1336, -0.2673,  0.2230,  ...,  0.1393, -0.0239, -0.1752],\n",
            "         [-0.0117, -0.3531,  0.3461,  ...,  0.0147, -0.2658, -0.0671]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.0331, -0.5197, -0.1171,  ...,  0.5891, -0.4349, -0.3353],\n",
            "         [ 0.1806, -0.4207,  0.1636,  ...,  0.4575,  0.3909, -0.1241],\n",
            "         [ 0.1254, -0.3076,  0.1102,  ...,  0.1346,  0.0173, -0.3472],\n",
            "         ...,\n",
            "         [ 0.1511, -0.1637,  0.0442,  ...,  0.2783,  0.1785, -0.2850],\n",
            "         [ 0.0607, -0.3474,  0.2028,  ...,  0.2565, -0.0852, -0.1350],\n",
            "         [-0.0179, -0.2643,  0.3676,  ...,  0.1262, -0.4255, -0.1941]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.0736, -0.1952,  0.0094,  ...,  0.6943, -0.3875, -0.0372],\n",
            "         [ 0.3043, -0.1925,  0.2230,  ...,  0.5090,  0.3864,  0.1194],\n",
            "         [ 0.1710, -0.0502,  0.0658,  ...,  0.1658, -0.1181, -0.4205],\n",
            "         ...,\n",
            "         [ 0.1438, -0.2793,  0.1675,  ...,  0.2942,  0.2153, -0.3679],\n",
            "         [ 0.1421, -0.2088,  0.0471,  ...,  0.1610, -0.0361, -0.2121],\n",
            "         [-0.0873, -0.2747,  0.4522,  ...,  0.4678, -0.4298, -0.2994]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[ 0.6101, -0.5301,  0.4406,  ...,  1.8482, -1.1944, -0.1890],\n",
            "         [ 1.3898, -0.4734,  1.0602,  ...,  1.3962,  0.6781,  0.3500],\n",
            "         [ 1.1229,  0.3377,  0.9552,  ...,  0.1074, -0.5983, -1.2665],\n",
            "         ...,\n",
            "         [ 0.9460, -0.4479,  0.8406,  ...,  0.7256,  0.2158, -1.4559],\n",
            "         [ 0.8252, -0.1307,  0.3311,  ...,  0.3360,  0.0356, -0.7334],\n",
            "         [ 0.0186, -0.6916,  1.6767,  ...,  0.5273, -1.3924, -0.7768]]],\n",
            "       grad_fn=<ViewBackward0>))\n"
          ]
        }
      ],
      "source": [
        "print(hidden_states_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "PwR-SAgnJWUl",
      "metadata": {
        "id": "PwR-SAgnJWUl"
      },
      "outputs": [],
      "source": [
        "hidden_states = hidden_states_all[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b51724e",
      "metadata": {
        "id": "9b51724e"
      },
      "source": [
        "Now we'll layer normalize. Our transformer has 6 attention heads, let's dive into one of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "df145980",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "df145980",
        "outputId": "c87b33ef-d02f-4103-b5d7-fcdbdbc1c991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnormalized:\n",
            "tensor([[[ 0.0736, -0.1952,  0.0094,  ...,  0.6943, -0.3875, -0.0372],\n",
            "         [ 0.3043, -0.1925,  0.2230,  ...,  0.5090,  0.3864,  0.1194],\n",
            "         [ 0.1710, -0.0502,  0.0658,  ...,  0.1658, -0.1181, -0.4205],\n",
            "         ...,\n",
            "         [ 0.1438, -0.2793,  0.1675,  ...,  0.2942,  0.2153, -0.3679],\n",
            "         [ 0.1421, -0.2088,  0.0471,  ...,  0.1610, -0.0361, -0.2121],\n",
            "         [-0.0873, -0.2747,  0.4522,  ...,  0.4678, -0.4298, -0.2994]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "-----------------\n",
            "Normalized:\n",
            "tensor([[[ 0.1932, -0.5694,  0.0111,  ...,  1.9544, -1.1152, -0.1211],\n",
            "         [ 0.9443, -0.6384,  0.6854,  ...,  1.5967,  1.2059,  0.3552],\n",
            "         [ 0.5724, -0.1586,  0.2250,  ...,  0.5554, -0.3830, -1.3823],\n",
            "         ...,\n",
            "         [ 0.5942, -1.0714,  0.6878,  ...,  1.1865,  0.8758, -1.4201],\n",
            "         [ 0.5864, -0.7640,  0.2207,  ...,  0.6589, -0.0993, -0.7769],\n",
            "         [-0.2997, -1.0198,  1.7729,  ...,  1.8328, -1.6154, -1.1145]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "normalized_hidden_states = model.transformer.h[0].ln_1(hidden_states)\n",
        "\n",
        "print(\"Unnormalized:\")\n",
        "print(hidden_states)\n",
        "print('-----------------')\n",
        "print(\"Normalized:\")\n",
        "print(normalized_hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf6ee66",
      "metadata": {
        "id": "baf6ee66"
      },
      "source": [
        "Let's take a look at the self-attention matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "00743869",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "00743869",
        "outputId": "22183014-408b-4ab4-8dd4-0a2f7f6bb65d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([768, 2304])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.transformer.h[0].attn.c_attn.weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47b5934",
      "metadata": {
        "id": "d47b5934"
      },
      "source": [
        "In class, we discussed the Wq, Wk, and Wv matrices. In practice, these are often stacked into a single matrix for efficient computation. So the matrix above represents [Wq Wk Wv].\n",
        "\n",
        "We can derive our Q, K, V matrices by splitting the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9a45ea4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9a45ea4e",
        "outputId": "55e80aee-0097-4948-943b-be51c71c09dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: tensor([[[-0.0176, -0.2581, -1.1487,  ..., -0.2125, -0.8392, -0.0996],\n",
            "         [-0.0337, -0.1256, -0.9511,  ...,  0.1579, -0.1324,  0.7849],\n",
            "         [-0.4454,  0.2913, -1.0810,  ..., -0.3892, -0.5600,  0.5538],\n",
            "         ...,\n",
            "         [-0.4624, -0.0847,  0.5706,  ...,  0.7240,  0.5958, -1.0402],\n",
            "         [ 0.1089, -0.1007,  0.2355,  ...,  0.3979,  0.6039, -0.1321],\n",
            "         [-0.7290,  0.3986, -0.3051,  ...,  0.0587, -0.2318,  0.5221]]],\n",
            "       grad_fn=<SplitBackward0>)\n",
            "K: tensor([[[ 0.8372, -0.5049,  0.4022,  ..., -0.1745, -0.8787,  0.3928],\n",
            "         [ 0.3752, -1.1121,  0.4688,  ...,  0.6258, -0.2741,  0.3571],\n",
            "         [ 0.7830, -0.3831,  0.6921,  ...,  0.4104, -0.3931,  0.7063],\n",
            "         ...,\n",
            "         [-0.2577,  0.0925,  0.5434,  ..., -0.6406, -0.0526,  0.0418],\n",
            "         [ 0.0377,  0.0011,  0.0932,  ...,  0.1470,  0.0587,  0.5681],\n",
            "         [ 0.5439,  0.7332,  0.4775,  ...,  0.0252, -0.2701,  0.8814]]],\n",
            "       grad_fn=<SplitBackward0>)\n",
            "V: tensor([[[ 0.4149, -0.1343,  0.4462,  ...,  0.2797, -0.0242,  0.5234],\n",
            "         [ 0.3533, -0.4630,  0.5855,  ...,  0.3836,  0.1795, -0.5911],\n",
            "         [ 0.2981,  0.4318,  0.9888,  ...,  0.0688, -0.4549,  0.6461],\n",
            "         ...,\n",
            "         [ 0.4899,  0.5130,  0.4613,  ...,  0.7303,  0.2618,  0.5522],\n",
            "         [-0.0082, -0.8687, -0.5609,  ...,  0.2540,  1.1445, -0.2806],\n",
            "         [ 0.4873, -0.3833, -0.2227,  ...,  0.1024, -0.1141, -0.0774]]],\n",
            "       grad_fn=<SplitBackward0>)\n"
          ]
        }
      ],
      "source": [
        "Q, K, V = model.transformer.h[0].attn.c_attn(normalized_hidden_states).split(768, dim=2)\n",
        "print('Q:', Q)\n",
        "print('K:', K)\n",
        "print('V:', V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a188ac2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a188ac2f",
        "outputId": "96b1f95b-9c8c-4e20-af7a-a09d3fc1f875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512, 768])\n"
          ]
        }
      ],
      "source": [
        "print(Q.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a96d67e4",
      "metadata": {
        "id": "a96d67e4"
      },
      "source": [
        "Remember, there's nothing special about these matrices/vectors. They are random. They only gain significance during training because the following constraint is applied during the forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "feb13dfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "feb13dfb",
        "outputId": "2d8fa53a-3152-4e80-e203-6d245749464e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QK^T Dim: torch.Size([1, 512, 512])\n",
            "Z Dim: torch.Size([1, 512, 768])\n"
          ]
        }
      ],
      "source": [
        "att = (Q @ K.transpose(-2, -1)) * (1.0 / math.sqrt(K.size(-1)))\n",
        "print(\"QK^T Dim:\", att.shape)\n",
        "A = F.softmax(att, dim=-1)\n",
        "Z = A @ V\n",
        "print(\"Z Dim:\", Z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3b0561",
      "metadata": {
        "id": "1e3b0561"
      },
      "source": [
        "Finally, the output of our model will be a matrix of logits (dimensionality of our vocabulary) for each position in the sequence. During training, we will compute the cross entropy between that logit and the embedding vectors of the next tokens (i.e. shifted by 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f474b25c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f474b25c",
        "outputId": "0a62de17-9bea-4f65-b2b3-0b9da495d0c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 50257])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(tokenized_input).logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e13ed3",
      "metadata": {
        "id": "36e13ed3"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c17db988",
      "metadata": {
        "id": "c17db988"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "53faf0e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "53faf0e8",
        "outputId": "2962ae03-3255-4268-ee8d-0df7d2b4644d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2972688972.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"lab3\",\n",
        "    per_device_train_batch_size=14, # increase/decrease this based on your memory\n",
        "    eval_steps=50,\n",
        "    logging_steps=50,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=10,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=500,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        "    # use_cpu=True # Very slow! Feel free to use without a GPU if you'd like\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SJMpc7f97cBL",
      "metadata": {
        "id": "SJMpc7f97cBL"
      },
      "source": [
        "The model will internally handle the next-token prediction loss. Go ahead and start the training. This will take a while!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5dbe2a18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2577
        },
        "id": "5dbe2a18",
        "outputId": "1749d0f3-abf2-4881-f1c1-aeb06ecd7dc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3926' max='3926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3926/3926 36:53, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>7.066700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.512800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.910300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.062500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.812300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.657800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.457300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.249700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.134800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.031000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.907400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.869000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.810700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.715300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.438600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>2.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>2.303600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.248400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>2.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.217300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>2.193300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.249100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>2.198100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>2.173700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.095900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>2.209700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>2.074700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>2.117000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>2.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.983800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>1.934800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.994500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>2.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.975700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>1.976700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.950200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>1.982500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.897800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>1.937700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.897700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>1.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.940600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>1.909000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>1.820700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.827000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>1.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>1.837200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.801600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>1.846400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.842700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>1.871000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.808600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>1.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.811300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>1.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>1.761800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.839000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>1.812100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.825900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>1.769800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.771300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>1.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.881100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3926, training_loss=2.399813412526889, metrics={'train_runtime': 2214.1298, 'train_samples_per_second': 24.814, 'train_steps_per_second': 1.773, 'total_flos': 7178083035512832.0, 'train_loss': 2.399813412526889, 'epoch': 2.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b74796",
      "metadata": {
        "id": "e5b74796"
      },
      "source": [
        "# Try again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7a20ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5c7a20ac",
        "outputId": "12e3b7f0-b40c-4ddc-f3c6-612132787a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' hospital', 'stay', 'Room', 'medical', 'home', 'event', 'different', 'community', 'past']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.encode('hospital')\n",
        "result_vector = model.transformer.wte.weight[tokens].mean(axis=0)\n",
        "\n",
        "similarities = F.cosine_similarity(\n",
        "    result_vector,\n",
        "    model.transformer.wte.weight\n",
        ")\n",
        "top_indices = similarities.topk(10).indices\n",
        "print([tokenizer.decode(idx) for idx in top_indices if idx not in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T2pm8sKGBHLZ",
      "metadata": {
        "id": "T2pm8sKGBHLZ"
      },
      "source": [
        "It's more related to hospital."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e939fa1",
      "metadata": {
        "id": "9e939fa1"
      },
      "source": [
        "# Use the trained model to generate  a few samples of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "70c60fe6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "70c60fe6",
        "outputId": "57245d6a-8c7b-49b2-c09a-eb97d97dde19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[50256,    13,   198,   198,  7155,   929, 27759,    25,   198,  5492,\n",
            "          1061,   510,   351,   534,  4217,    47,   685,  1174,  5956,  6530,\n",
            "           357,  5376, 47546,    19,     8, 12429,  4083,   685,  1174,  5956,\n",
            "          6530,   357,  2257,  2578,     8, 12429,    60,   319,   685,  1174,\n",
            "            17, 21652,    12,    23,    12,    23,  1174,    60,   379,  1367,\n",
            "            25,  1270,  3001,    13,   198,   198,  5492,  1061,   510,   351,\n",
            "          1583,    13,   685,  1174,  5956,  6530,   357,  2257,  2578,     8,\n",
            "         12429,    60,   379,   685,  1174, 31709,  4862,    14, 46512,   357,\n",
            "            16,     8,   718,  1065,  1174,    60,   198,   198,  5492,  1061,\n",
            "           510,   351,   534,  4165,  1337,  6253,    11,   685,  1174,  5956]],\n",
            "       device='cuda:0')\n",
            "<|endoftext|>.\n",
            "\n",
            "Followup Instructions:\n",
            "Please follow up with your PCP [**Last Name (NamePattern4) **]. [**Last Name (STitle) **] on [**2185-8-8**] at 11:30 AM.\n",
            "\n",
            "Please follow up with Dr. [**Last Name (STitle) **] at [**Telephone/Fax (1) 612**]\n",
            "\n",
            "Please follow up with your primary care doctor, [**Last\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# model.generate(max_length=100, do_sample=True, temperature=0.1)\n",
        "output=model.generate(max_length=100, do_sample=True, temperature=0.1)\n",
        "print(output)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bWyylYV1BL4c",
      "metadata": {
        "id": "bWyylYV1BL4c"
      },
      "source": [
        "It's more like a discharge summary."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033d47b1f05546139546b3657093b143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3422602e7c847aaa1a108d5bcdcb883",
            "max": 3668,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a9bca42f4547fdaeb874d0fce6fdf7",
            "value": 3668
          }
        },
        "33ad13fd4b4a40aa9f5eac0331317633": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e2c7506bf34a768b0818e883964c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eab694a9ab14e87bd3ada7a9b5f4d23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8291003794e34c969830ce0b5f88ec8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9184b51614c24be68cff97eddd5db38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9764e73924147a29464f09db0d168ec",
              "IPY_MODEL_033d47b1f05546139546b3657093b143",
              "IPY_MODEL_aace702e4a6745f88abbf3b0d5768990"
            ],
            "layout": "IPY_MODEL_8291003794e34c969830ce0b5f88ec8f"
          }
        },
        "aace702e4a6745f88abbf3b0d5768990": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eab694a9ab14e87bd3ada7a9b5f4d23",
            "placeholder": "​",
            "style": "IPY_MODEL_dd3c90bd54ed4a27a70596130b78721f",
            "value": " 3668/3668 [00:32&lt;00:00, 104.17 examples/s]"
          }
        },
        "b0a9bca42f4547fdaeb874d0fce6fdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3422602e7c847aaa1a108d5bcdcb883": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9764e73924147a29464f09db0d168ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ad13fd4b4a40aa9f5eac0331317633",
            "placeholder": "​",
            "style": "IPY_MODEL_34e2c7506bf34a768b0818e883964c3d",
            "value": "Map: 100%"
          }
        },
        "dd3c90bd54ed4a27a70596130b78721f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
