{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "4dae1420"
   },
   "source": [
    "# Training GPT from Scratch on Discharge Summaries\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this lab, we will walk through a slightly modified version of this huggingface tutorial on training GPT from scratch: https://huggingface.co/learn/llm-course/en/chapter7/6\n",
    "\n",
    "We'll explore how training on a small corpus of discharge summaries affects the embeddings and generations of our model.\n",
    "\n",
    "We'll primarily use the higher level \"transformers\" API for this exercise. \n",
    "\n",
    "This notebook runs in colab, connect to a T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fUdJrc-Up5kH",
    "outputId": "0860ae27-b991-4cd4-a8e3-524a4e57ac14"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "9c8e2ce0"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "abbcbb7b"
   },
   "source": [
    "First, we'll load the data that we used in Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 78
    },
    "id": "NK9G-FnTq0vF",
    "outputId": "33b45c82-c659-4593-92d6-0e50a5f54404"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3ffa99da",
    "outputId": "34b9b8ce-6b2d-4de2-d0ad-f96396f741de"
   },
   "outputs": [],
   "source": [
    "discharge_summaries = pd.read_csv('lab2-data.csv')\n",
    "dataset = Dataset.from_pandas(discharge_summaries)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "090f2279"
   },
   "source": [
    "Here, we'll use a pre-trained tokenizer. We'll also restrict our context length to 512. The tokenizer class allows us to break our large discharge summaries into smaller chunks that fit into our context limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "49a179a6",
    "outputId": "3a6df285-0859-4359-f0f0-929fdd8f96e7"
   },
   "outputs": [],
   "source": [
    "context_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    dataset[:10][\"TEXT\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "30f8222e"
   },
   "source": [
    "Each chunk will be list of indices corresponding to tokens in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "553036eb",
    "outputId": "2abd3e24-413b-4560-d56f-aa38dcbfe0d4"
   },
   "outputs": [],
   "source": [
    "print(outputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "5741ec77"
   },
   "source": [
    "We can decode this back to text as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "32ff3639",
    "outputId": "2bc98356-1d30-419d-c85d-897868125f22"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "e2f6a26d"
   },
   "source": [
    "Let's now apply the tokenizer across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9184b51614c24be68cff97eddd5db38f",
      "d9764e73924147a29464f09db0d168ec",
      "033d47b1f05546139546b3657093b143",
      "aace702e4a6745f88abbf3b0d5768990",
      "8291003794e34c969830ce0b5f88ec8f",
      "33ad13fd4b4a40aa9f5eac0331317633",
      "34e2c7506bf34a768b0818e883964c3d",
      "c3422602e7c847aaa1a108d5bcdcb883",
      "b0a9bca42f4547fdaeb874d0fce6fdf7",
      "3eab694a9ab14e87bd3ada7a9b5f4d23",
      "dd3c90bd54ed4a27a70596130b78721f"
     ]
    },
    "id": "ac5f6f31",
    "outputId": "960d491d-97c3-4529-966d-4db59e4e092d"
   },
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"TEXT\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return {\"input_ids\": outputs['input_ids']}\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "cab4e574"
   },
   "source": [
    "Now we'll load in a randomly initialized model with the GPT2 architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "8b7916dc"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    n_layer=6,\n",
    "    n_head=6\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "87d79db4"
   },
   "source": [
    "Let's explore this architecture a bit further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "520c34c2",
    "outputId": "9ad16bde-17a6-4ba4-fdf1-4580a9150b1c"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "e4b76c7c"
   },
   "source": [
    "WTE (Word Token Embeddings) maps each vocabulary token to a 768-dimensional semantic vector, while WPE (Word Position Embeddings) encodes each token’s position in the sequence so the model can capture word order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "25b3ee68"
   },
   "source": [
    "We can can search for similar tokens using vector distances in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fdf83526",
    "outputId": "c0b295fa-1abe-46d3-dc4c-04a7820de715"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode('hospital')\n",
    "result_vector = model.transformer.wte.weight[tokens].mean(axis=0)\n",
    "\n",
    "similarities = F.cosine_similarity(\n",
    "    result_vector,\n",
    "    model.transformer.wte.weight\n",
    ")\n",
    "top_indices = similarities.topk(10).indices\n",
    "print([tokenizer.decode(idx) for idx in top_indices if idx not in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "0f7e8bcf"
   },
   "source": [
    "These tokens are not quite related to “hospital”.\n",
    "\n",
    "Because we are using the raw word embedding table that is not trained to make similar words close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "452bf39d"
   },
   "source": [
    "# Use this model to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ea6dfd6c",
    "outputId": "a39c4694-1cee-448c-8529-4f02208e8966"
   },
   "outputs": [],
   "source": [
    "output=model.generate(max_length=100)\n",
    "print(output)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "yKZT92folz85"
   },
   "source": [
    "The output is repeating and meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "787cf6e1"
   },
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "acbec172"
   },
   "source": [
    "Let's pass one of our input vectors in and explore the outputs.\n",
    "\n",
    "First, we'll pass our tokenized input and retrieve the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "d9012afd",
    "outputId": "37782d9f-fcec-4d9d-98e7-56935f821c82"
   },
   "outputs": [],
   "source": [
    "tokenized_input = torch.tensor(outputs['input_ids'][0:1])\n",
    "token_embeddings = model.transformer.wte(tokenized_input)\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "1d8b3a09"
   },
   "source": [
    "# Generate the position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "83b7b433",
    "outputId": "418c2ef4-04b4-4ed4-d5a2-cba3eb08dcc4"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    seq_len = tokenized_input.size(1)\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "    position_embeddings = model.transformer.wpe(position_ids)\n",
    "\n",
    "print(position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "2c2ce13e"
   },
   "source": [
    "# Combine the position and token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "92db8895",
    "outputId": "b5f4aad5-b122-4918-e176-8fecd88f7d21"
   },
   "outputs": [],
   "source": [
    "inputs_embeds = token_embeddings + position_embeddings\n",
    "print(inputs_embeds)\n",
    "outputs = model(inputs_embeds=inputs_embeds, output_hidden_states=True)\n",
    "hidden_states_all = outputs.hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1dzHf_JfKsGH",
    "outputId": "880435be-924f-488b-d870-6ebe39717438"
   },
   "outputs": [],
   "source": [
    "print(hidden_states_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "id": "PwR-SAgnJWUl"
   },
   "outputs": [],
   "source": [
    "hidden_states = hidden_states_all[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "9b51724e"
   },
   "source": [
    "Now we'll layer normalize. Our transformer has 6 attention heads, let's dive into one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "df145980",
    "outputId": "c87b33ef-d02f-4103-b5d7-fcdbdbc1c991"
   },
   "outputs": [],
   "source": [
    "normalized_hidden_states = model.transformer.h[0].ln_1(hidden_states)\n",
    "\n",
    "print(\"Unnormalized:\")\n",
    "print(hidden_states)\n",
    "print('-----------------')\n",
    "print(\"Normalized:\")\n",
    "print(normalized_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "baf6ee66"
   },
   "source": [
    "Let's take a look at the self-attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "00743869",
    "outputId": "22183014-408b-4ab4-8dd4-0a2f7f6bb65d"
   },
   "outputs": [],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "d47b5934"
   },
   "source": [
    "In class, we discussed the Wq, Wk, and Wv matrices. In practice, these are often stacked into a single matrix for efficient computation. So the matrix above represents [Wq Wk Wv].\n",
    "\n",
    "We can derive our Q, K, V matrices by splitting the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "9a45ea4e",
    "outputId": "55e80aee-0097-4948-943b-be51c71c09dc"
   },
   "outputs": [],
   "source": [
    "Q, K, V = model.transformer.h[0].attn.c_attn(normalized_hidden_states).split(768, dim=2)\n",
    "print('Q:', Q)\n",
    "print('K:', K)\n",
    "print('V:', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "a188ac2f",
    "outputId": "96b1f95b-9c8c-4e20-af7a-a09d3fc1f875"
   },
   "outputs": [],
   "source": [
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "a96d67e4"
   },
   "source": [
    "Remember, there's nothing special about these matrices/vectors. They are random. They only gain significance during training because the following constraint is applied during the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "feb13dfb",
    "outputId": "2d8fa53a-3152-4e80-e203-6d245749464e"
   },
   "outputs": [],
   "source": [
    "att = (Q @ K.transpose(-2, -1)) * (1.0 / math.sqrt(K.size(-1)))\n",
    "print(\"QK^T Dim:\", att.shape)\n",
    "A = F.softmax(att, dim=-1)\n",
    "Z = A @ V\n",
    "print(\"Z Dim:\", Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "1e3b0561"
   },
   "source": [
    "Finally, the output of our model will be a matrix of logits (dimensionality of our vocabulary) for each position in the sequence. During training, we will compute the cross entropy between that logit and the embedding vectors of the next tokens (i.e. shifted by 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f474b25c",
    "outputId": "0a62de17-9bea-4f65-b2b3-0b9da495d0c4"
   },
   "outputs": [],
   "source": [
    "model(tokenized_input).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "36e13ed3"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "id": "c17db988"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "53faf0e8",
    "outputId": "2962ae03-3255-4268-ee8d-0df7d2b4644d"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"lab3\",\n",
    "    per_device_train_batch_size=14, # increase/decrease this based on your memory\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=10,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    "    # use_cpu=True # Very slow! Feel free to use without a GPU if you'd like\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "SJMpc7f97cBL"
   },
   "source": [
    "The model will internally handle the next-token prediction loss. Go ahead and start the training. This will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2577
    },
    "id": "5dbe2a18",
    "outputId": "1749d0f3-abf2-4881-f1c1-aeb06ecd7dc8"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "e5b74796"
   },
   "source": [
    "# Try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5c7a20ac",
    "outputId": "12e3b7f0-b40c-4ddc-f3c6-612132787a63"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode('hospital')\n",
    "result_vector = model.transformer.wte.weight[tokens].mean(axis=0)\n",
    "\n",
    "similarities = F.cosine_similarity(\n",
    "    result_vector,\n",
    "    model.transformer.wte.weight\n",
    ")\n",
    "top_indices = similarities.topk(10).indices\n",
    "print([tokenizer.decode(idx) for idx in top_indices if idx not in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "T2pm8sKGBHLZ"
   },
   "source": [
    "It's more related to hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "9e939fa1"
   },
   "source": [
    "# Use the trained model to generate  a few samples of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "70c60fe6",
    "outputId": "57245d6a-8c7b-49b2-c09a-eb97d97dde19"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# model.generate(max_length=100, do_sample=True, temperature=0.1)\n",
    "output=model.generate(max_length=100, do_sample=True, temperature=0.1)\n",
    "print(output)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "bWyylYV1BL4c"
   },
   "source": [
    "It's more like a discharge summary."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
